# ==============================================================================
# ğŸ“Œ ç­–ç•¥æ± ç”Ÿæˆå™¨ (src/core/pool_generator.py) - ã€ç›˜åè¿è¡Œã€‘
# Version: 1.1 | Last Modified: 2026-01-11
# ==============================================================================
import pandas as pd
import os
import shutil
import sys
import re
from datetime import datetime
import json
from colorama import init, Fore

# --- å¯¼å…¥ä¿®å¤ ---
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

from data_loader import get_merged_data, load_yesterday_ths_data
from market_data import MarketDataManager

# Add project root to path for strategies import if needed
# But assume standard import works if we fix the paths later or relies on existing sys.path
try:
    from strategies.f_lao_model import load_ths_history, check_fen_jue
except ImportError:
    # Fallback if run from different dir
    sys.path.append(os.path.join(os.path.dirname(os.path.dirname(current_dir)), 'src')) 
    from strategies.f_lao_model import load_ths_history, check_fen_jue
# --------------

init(autoreset=True)

# ================= 1. è·¯å¾„ä¸é…ç½® =================

PROJECT_ROOT = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(PROJECT_ROOT) # Fix import src issue

OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'data', 'output')
ARCHIVE_DIR = os.path.join(OUTPUT_DIR, 'archive')

HOLDINGS_PATH = os.path.join(PROJECT_ROOT, 'data', 'input', 'holdings.txt')
F_LAO_PATH = os.path.join(PROJECT_ROOT, 'data', 'input', 'f_lao_list.txt')

# --- ç­–ç•¥é…ç½® (ä¸ akshare ç‰ˆä¿æŒä¸€è‡´) ---
CORE_KEYWORDS = [
    'æœºå™¨äºº', 'èˆªå¤©', 'å†›å·¥', 'å«æ˜Ÿ', 'ä½ç©º',
    'AI', 'äººå·¥æ™ºèƒ½', 'æ™ºèƒ½ä½“', 'ç®—åŠ›', 'CPO', 'å­˜å‚¨',
    'æ¶ˆè´¹ç”µå­', 'åä¸º', 'ä¿¡åˆ›', 'æ•°å­—è´§å¸', 'æ•°æ®è¦ç´ ',
    'æ–‡åŒ–ä¼ åª’', 'çŸ­å‰§', 'å¤šæ¨¡æ€', 'çººç»‡', 'å¹¶è´­é‡ç»„', 'å›ºæ€ç”µæ± ', 'è‡ªåŠ¨é©¾é©¶'
]

# æŒä»“è‚¡ç‰¹æ®Šç­–ç•¥é…ç½® (ä»£ç : (æ ‡ç­¾, è”åŠ¨å¤§å“¥ä»£ç ))
HOLDING_STRATEGIES = {
}

# è”åŠ¨å¤§å“¥æ˜ å°„ (å°å¼Ÿä»£ç : å¤§å“¥ä»£ç )
LINK_DRAGON_MAP = {
    '002009': '002931',
}


# ================= 2. è¾…åŠ©å‡½æ•° =================

def load_text_list(filepath):
    """åŠ è½½å…³æ³¨åˆ—è¡¨/æŒä»“åˆ—è¡¨"""
    if not os.path.exists(filepath): return {}
    mapping = {}
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'): continue
                parts = re.split(r'\s+', line, maxsplit=1)
                code = parts[0].strip()
                # ç®€å•æ¸…æ´—ä»£ç 
                code = code.replace("SZ", "").replace("SH", "")
                if code.isdigit() and len(code) == 6:
                    tag = parts[1].strip() if len(parts) > 1 else "å…³æ³¨"
                    mapping[code] = tag
    except Exception as e:
        print(f"{Fore.RED}åŠ è½½åˆ—è¡¨å¤±è´¥ {filepath}: {e}")
    return mapping


def load_yesterday_pool():
    """
    åŠ è½½æœ€è¿‘ä¸€æœŸçš„ç­–ç•¥æ± æ–‡ä»¶ (ä¸å«ä»Šæ—¥)
    ç›®çš„æ˜¯å¯»æ‰¾æ˜¨æ—¥ç‚¸æ¿çš„è‚¡ç¥¨
    è¿”å›: {code: {'amount': float, 'tag': str}}
    """
    if not os.path.exists(OUTPUT_DIR): return {}
    
    # 1. æŸ¥æ‰¾æ‰€æœ‰ strategy_pool_YYYYMMDD.csv
    files = []
    today_str = datetime.now().strftime("%Y%m%d")
    
    for f in os.listdir(OUTPUT_DIR):
        if f.startswith('strategy_pool_') and f.endswith('.csv'):
            date_part = f.replace('strategy_pool_', '').replace('.csv', '')
            if date_part.isdigit() and date_part < today_str:
                files.append({'path': os.path.join(OUTPUT_DIR, f), 'date': date_part})
                
    if not files:
        # å°è¯• archive ç›®å½•
        if os.path.exists(ARCHIVE_DIR):
            for f in os.listdir(ARCHIVE_DIR):
                if f.startswith('strategy_pool_') and f.endswith('.csv'):
                    date_part = f.replace('strategy_pool_', '').replace('.csv', '')
                    if date_part.isdigit() and date_part < today_str:
                        files.append({'path': os.path.join(ARCHIVE_DIR, f), 'date': date_part})

    if not files:
        print(f"{Fore.YELLOW}âš ï¸ æœªæ‰¾åˆ°æ˜¨æ—¥(æˆ–æ›´æ—©)çš„ç­–ç•¥æ± æ–‡ä»¶ï¼Œæ— æ³•æ‰§è¡Œ[æ–­æ¿ååŒ…]ç­–ç•¥")
        return {}
        
    # 2. æ’åºå–æœ€æ–°çš„ä¸€ä¸ª
    files.sort(key=lambda x: x['date'], reverse=True)
    target_file = files[0]['path']
    print(f"{Fore.BLUE}ğŸ”™ å›æº¯å†å²æ•°æ®: {os.path.basename(target_file)}")
    
    res_map = {}
    try:
        df = pd.read_csv(target_file, dtype={'code': str, 'sina_code': str})
        # å¿…é¡»åˆ—: code, tag, amount
        for _, row in df.iterrows():
            c = str(row['code']).zfill(6)
            tag = str(row.get('tag', ''))
            
            # ç­›é€‰æ˜¨æ—¥ç‚¸æ¿è‚¡ (tagä¸­åŒ…å«"ç‚¸æ¿")
            # æ³¨æ„ï¼šæ˜¨æ—¥å¿…é¡»æ˜¯çœŸçš„ç‚¸æ¿äº†ï¼Œè€Œä¸æ˜¯"ååŒ…é¢„æœŸ"è¿™ç§
            # ç®€å•åˆ¤æ–­: åªè¦ tag é‡Œæœ‰ "ç‚¸æ¿" å­—æ ·ï¼Œå°±çº³å…¥è§‚å¯Ÿæ± 
            if "ç‚¸æ¿" in tag:
                res_map[c] = {
                    'amount': float(row.get('amount', 0)),
                    'tag': tag
                }
    except Exception as e:
        print(f"{Fore.RED}âŒ è¯»å–å†å²æ–‡ä»¶å¤±è´¥: {e}")
        
    return res_map


def load_lhb_info():
    """
    åŠ è½½é¾™è™æ¦œæ•°æ® & æ¸¸èµ„æ•°æ®
    Returns:
       lhb_codes: set of codes (str 6 digits)
       seat_map: {stock_name: [tags]}
    """
    lhb_dir = os.path.join(PROJECT_ROOT, 'data', 'output', 'lhb')
    lhb_path = os.path.join(lhb_dir, 'lhb_latest.csv')
    seat_path = os.path.join(lhb_dir, 'lhb_famous_latest.csv')
    
    lhb_codes = set()
    if os.path.exists(lhb_path):
        try:
             df = pd.read_csv(lhb_path, dtype=str)
             # åŒæ ·æ¸…æ´—ä¸‹ input
             if 'ä»£ç ' in df.columns:
                 # ç¡®ä¿æ˜¯6ä½
                 lhb_codes = set(df['ä»£ç '].apply(lambda x: str(x).strip().zfill(6)).tolist())
        except Exception as e:
            print(f"{Fore.RED}âŒ LHBåŠ è½½å¤±è´¥: {e}")

    seat_map = {}
    if os.path.exists(seat_path):
         try:
             df = pd.read_csv(seat_path, dtype=str)
             import re
             
             # Columns: æ¸¸èµ„æ ‡ç­¾, è¥ä¸šéƒ¨åç§°, ä¹°å…¥è‚¡ç¥¨, å–å‡ºè‚¡ç¥¨...
             for _, row in df.iterrows():
                 label = row['æ¸¸èµ„æ ‡ç­¾']
                 
                 # Helper to process string: "Stock(1äº¿) Stock/3æ—¥(2äº¿)"
                 def parse_lhb_str(raw_str, default_prefix):
                    if not raw_str or raw_str == 'nan': return
                    # Split by space
                    parts = raw_str.split(' ')
                    for p in parts:
                        p = p.strip()
                        if not p: continue
                        
                        s_name = p
                        note = ""
                        
                        if '(' in p:
                            s_name = p.split('(')[0]
                            # Capture content inside parenthesis, e.g. (1äº¿) or (ğŸ”’ é”ä»“)
                            content = p.split('(')[1].rstrip(')')
                            note = f"({content})"
                        
                        # Handle /Tag in name
                        tag_info = ""
                        if '/' in s_name:
                            real_name = s_name.split('/')[0]
                            tag_part = s_name.split('/')[1] # e.g. 3æ—¥
                            s_name = real_name
                            tag_info = f"/{tag_part}"
                        
                        if s_name not in seat_map: seat_map[s_name] = set()
                        
                        # Determine prefix based on content
                        prefix = default_prefix
                        if "é”ä»“" in note or "é”ä»“" in p:
                            prefix = "ğŸ”’" # Lock
                        elif "åŠ ä»“" in note:
                            prefix = "â•" # Add (Stronger than Buy)
                        
                        # Construct tag
                        full_tag = f"{prefix}{label}{tag_info}{note}"
                        seat_map[s_name].add(full_tag)

                 parse_lhb_str(str(row.get('ä¹°å…¥è‚¡ç¥¨', '')), "ğŸ’°")
                 parse_lhb_str(str(row.get('å–å‡ºè‚¡ç¥¨', '')), "ğŸƒ")
                     
         except Exception as e:
            print(f"{Fore.RED}âŒ æ¸¸èµ„æ•°æ®åŠ è½½å¤±è´¥: {e}")
            
    return lhb_codes, seat_map



def format_sina(code):
    code = str(code)
    if code.startswith('6'): return f"sh{code}"
    if code.startswith('8') or code.startswith('4'): return f"bj{code}"
    return f"sz{code}"


def get_link_dragon(code):
    """è·å–å…³è”çš„å¤§å“¥ä»£ç """
    if code in HOLDING_STRATEGIES:
        dragon = HOLDING_STRATEGIES[code][1]
        if dragon: return dragon

    dragon = LINK_DRAGON_MAP.get(code, '')
    if dragon:
        if dragon.startswith('sz') or dragon.startswith('sh'): return dragon
        return format_sina(dragon)
    return ''


def clean_manual_tag(tag, is_zt_tag_present):
    """
    æ¸…æ´—æ‰‹åŠ¨æ ‡ç­¾ï¼Œé¿å…å†—ä½™
    1. å»é™¤é‡å¤çš„ 'Fä½¬' å‰ç¼€
    2. å¦‚æœæœ‰å®æ—¶æ¶¨åœæ•°æ®ï¼Œå°è¯•ç§»é™¤æ‰‹åŠ¨å¤‡æ³¨ä¸­è¿‡æ—¶çš„ '3æ¿' ç­‰å­—æ ·
    """
    if not tag: return ""

    # 1. æ¸…ç†é‡å¤å‰ç¼€ (å¦‚ "Fä½¬/Fä½¬/...")
    if tag.startswith("Fä½¬/"):
        tag = tag[3:]
    elif tag.startswith("Fä½¬"):
        tag = tag.lstrip("Fä½¬").lstrip("/")

    # 2. æ¸…ç†è¿‡æ—¶è¿æ¿ä¿¡æ¯ (e.g. å¤‡æ³¨æ˜¯3æ¿ï¼Œä½†ä»Šå¤©å®é™…ä¸Š4æ¿äº†)
    if is_zt_tag_present:
        # æ­£åˆ™æ›¿æ¢ï¼šåŒ¹é… "3æ¿", "2è¿æ¿" ç­‰ï¼Œä¸”å‰åæœ‰åˆ†éš”ç¬¦æˆ–è¾¹ç•Œ
        # å…¼å®¹ "é›·ç§‘(3æ¿/å†›å·¥)" è¿™ç§æ‹¬å·å†…çš„å†™æ³•
        tag = re.sub(r'(^|/|[(])\d+æ¿([)]|/|$)', r'\1\2', tag)

        # æ¸…ç†æ­£åˆ™æ›¿æ¢åç•™ä¸‹çš„æ®‹ç•™ç¬¦å· (å¦‚ "//", "()")
        tag = tag.replace('()', '').replace('//', '/').replace('(/', '(').replace('/)', ')')
        tag = tag.strip('/')

    return tag


def get_unique_concepts(base_str, new_concepts_str):
    """
    ä»…è¿”å› base_str (æ‰‹åŠ¨å¤‡æ³¨) ä¸­ä¸å­˜åœ¨çš„æ–°æ¦‚å¿µ
    é¿å…å‡ºç° "é›·ç§‘(å†›å·¥)/.../å†›å·¥" è¿™ç§é‡å¤
    """
    if not new_concepts_str: return ""

    # å°† base_str æ‹†è§£ä¸ºå…³é”®è¯é›†åˆ (æŒ‰ / å’Œ æ‹¬å· æ‹†åˆ†)
    base_parts = re.split(r'[/()]', base_str)
    base_set = set(p.strip() for p in base_parts if p.strip())

    new_parts = new_concepts_str.split('/')
    final_new = []
    for c in new_parts:
        c = c.strip()
        # å¦‚æœæ–°æ¦‚å¿µä¸åœ¨å·²æœ‰é›†åˆä¸­ï¼Œä¸”ä¸æ˜¯å·²æœ‰å­—ç¬¦ä¸²çš„å­ä¸² (é˜²æ­¢ "å†›å·¥" vs "å†›å·¥æ¿å—" é‡å¤)
        if c and c not in base_set and c not in base_str:
            final_new.append(c)

    return "/".join(final_new)


def get_core_concepts_local(name, raw_tag):
    """æœ¬åœ°æå–æ ¸å¿ƒæ¦‚å¿µ"""
    matched = set()
    source_text = f"{name} {raw_tag}"

    for key in CORE_KEYWORDS:
        if key in source_text:
            matched.add(key)

    return "/".join(list(matched))




# --- New Logic: Calculate Sector & Sentiment ---

def calculate_market_stats(all_data, yesterday_data):
    """
    è®¡ç®—: 
    1. æ¶¨è·Œåœå®¶æ•° (éST)
    2. æ˜¨æ—¥æ¶¨åœæº¢ä»·
    3. è¿æ¿é«˜åº¦
    
    * æ¿å—æ¶¨å¹…/èµ„é‡‘æµå‘æ•°æ®ç°åœ¨ç”± MarketDataManager ç›´æ¥è¯»å– ths æ–‡ä»¶æä¾›
    """
    stats = {}
    
    # --- 1. Limit Up/Down Counts ---
    limit_up = 0
    limit_down = 0
    max_height = 0
    
    for item in all_data:
        name = item['name']
        if 'ST' in name.upper(): continue
        
        pct = item.get('today_pct', 0)
        
        # Simple ZT/DT check (approximate)
        if pct > 9.8: limit_up += 1
        if pct < -9.0: limit_down += 1
        
        h = item.get('limit_days', 0)
        if h > max_height: max_height = h
        
    stats['limit_up_count'] = limit_up
    stats['limit_down_count'] = limit_down
    stats['highest_space'] = max_height
    
    # --- 2. Yesterday ZT Premium ---
    # Find stocks that were ZT yesterday
    yest_zt_codes = [c for c, v in yesterday_data.items() if v.get('is_zt')]
    
    total_premium = 0
    valid_premium_count = 0
    for c in yest_zt_codes:
        # Check current performance
        # need to find item in all_data by code
        curr = next((x for x in all_data if x['code'] == c), None)
        if curr:
            total_premium += curr.get('open_pct', 0)
            valid_premium_count += 1
            
    avg_premium = round(total_premium / valid_premium_count, 2) if valid_premium_count > 0 else 0
    stats['yesterday_limit_up_premium'] = avg_premium
    
    return stats


def check_special_shape(item):
    """æ£€æŸ¥ç‰¹æ®Šå½¢æ€ (åœ°å¤©æ¿/20cm/èµ„é‡‘é¢)"""
    tags = []
    pct = item.get('today_pct', 0)
    # ... (existing logic kept but refactored into this function? No, function exists, just verify)
    # Original function body was small, I will just keep the original valid.
    # Wait, tool calling 'replace' with context. The original function is below.
    # I will just REPLACE the original function if I want to update it, or just INSERT above.
    
    # New Logic: Limit Up Type
    limit_type = ""
    if item.get('is_zt'):
        open_pct = item.get('open_pct', 0)
        open_num = item.get('open_num', 0)
        
        if open_pct > 9.0:
            if open_num == 0:
                limit_type = "ä¸€å­—"
            else:
                limit_type = "Tå­—"
        else:
             limit_type = "æ¢æ‰‹æ¿"
             
        if open_num > 5: # Many opens
            limit_type += "/çƒ‚æ¿"
            
    return tags, limit_type




# ================= 3. ä¸»ç”Ÿæˆé€»è¾‘ =================

def generate_strategy_pool():
    all_data = get_merged_data()
    if not all_data:
        print(f"{Fore.RED}âŒ æ•°æ®æºä¸ºç©ºï¼Œè¯·æ£€æŸ¥ data_loader")
        return

    holdings_map = load_text_list(HOLDINGS_PATH)
    f_lao_map = load_text_list(F_LAO_PATH)
    
    # --- è¾¨è¯†åº¦/äººæ°”æ ‡çš„åŠ è½½ ---
    MANUAL_FOCUS_PATH = os.path.join(PROJECT_ROOT, 'data', 'input', 'manual_focus.txt')
    manual_recognition_map = load_text_list(MANUAL_FOCUS_PATH)

    # --- æ˜¨æ—¥ç‚¸æ¿æ•°æ®åŠ è½½ (æ–°ç­–ç•¥) ---
    broken_pool_map = load_yesterday_pool()
    
    # --- é¾™è™æ¦œ/æ¸¸èµ„æ•°æ®åŠ è½½ (æ–°ç­–ç•¥) ---
    lhb_codes, lhb_seat_map = load_lhb_info()

    # --- æ˜¨æ—¥å®Œæ•´æ•°æ®åŠ è½½ for Premium & Ratio ---
    print(f"{Fore.MAGENTA}ğŸ”™ æ­£åœ¨åŠ è½½æ˜¨æ—¥å…¨é‡æ•°æ®ä»¥è®¡ç®—ç«ä»·/æº¢ä»·...")
    yest_full_data = load_yesterday_ths_data()

    # --- å¤§ç›˜/æƒ…ç»ªæ•°æ®åŠ è½½ (New) ---
    dapan_dir = os.path.join(PROJECT_ROOT, 'data', 'input', 'dapan')
    md_manager = MarketDataManager(dapan_dir)
    market_loaded = md_manager.load_data()
    
    # --- Fä½¬æ¨¡å‹å†å²æ•°æ®åŠ è½½ (New) ---
    print(f"{Fore.MAGENTA}ï¿½ æ­£åœ¨åŠ è½½æœ€è¿‘5æ—¥å†å²æ•°æ® (for Fä½¬æ¨¡å‹)...")
    ths_input_dir = os.path.join(PROJECT_ROOT, 'data', 'input', 'ths')
    history_map = load_ths_history(ths_input_dir, days=5)
    
    # Calculate enhanced stats
    market_stats = calculate_market_stats(all_data, yest_full_data)
    md_manager.update_extra_stats(market_stats) # Implicitly assume MarketDataManager can hold this, or just merge into final json
    
    if market_loaded:
        print(f"   âœ… {md_manager.get_formatted_summary()}")
    else:
        print(f"   âš ï¸ warning: æœªæ‰¾åˆ°å¤§ç›˜æ•°æ®")

    # åˆå¹¶åŸºæœ¬å…³æ³¨ï¼ˆFä½¬ + æŒä»“ï¼‰
    base_focus = f_lao_map.copy()
    base_focus.update(holdings_map)

    print(f"{Fore.CYAN}ğŸ“‹ ç¦»çº¿ç”Ÿæˆå¯åŠ¨ | æ•°æ®æº: {len(all_data)}æ¡ | æŒä»“: {len(holdings_map)} | å…³æ³¨: {len(f_lao_map)} | LHB: {len(lhb_codes)}")

    pool = []

    for item in all_data:
        code = str(item['code'])
        name = item['name']
        pct = item.get('today_pct', 0)

        raw_tag_str = str(item.get('tag', ''))
        if 'nan' in raw_tag_str: raw_tag_str = ""
        
        # --- 0. å…¨å±€è¿‡æ»¤: å‰”é™¤ ST è‚¡ ---
        if 'ST' in name.upper():
            continue

        base_tags = []
        is_selected = False
        has_zt_status = False  # æ˜¯å¦æœ‰æ¶¨åœçŠ¶æ€

        # --- 1. æ¶¨åœçŠ¶æ€é¢„åˆ¤ ---
        # å…ˆåˆ¤æ–­æ¶¨åœï¼Œæ–¹ä¾¿åç»­æ¸…æ´—æ‰‹åŠ¨æ ‡ç­¾æ—¶çŸ¥é“æ˜¯å¦è¦ç§»é™¤æ—§æ¿æ•°
        is_zt = item.get('is_zt') or (pct >= 9.8)
        zt_tag = ""
        if is_zt:
            has_zt_status = True
            limit_days = item.get('limit_days', 0)
            zt_tag = f"{limit_days}æ¿" if limit_days > 0 else "é¦–æ¿"
            open_num = item.get('open_num', 0)
            if open_num > 0:
                zt_tag += f"/å›å°(ç‚¸{open_num}æ¬¡)"
            elif item.get('is_first_limit'):
                zt_tag += "/ç¡¬æ¿"

        # --- 2. èº«ä»½åˆ¤å®š (æŒä»“/å…³æ³¨) ---
        manual_cleaned_tag = ""
        if code in base_focus:
            is_selected = True
            if code in HOLDING_STRATEGIES:
                # ç‰¹æ®Šç­–ç•¥ï¼Œç›´æ¥ä½¿ç”¨
                base_tags.append(HOLDING_STRATEGIES[code][0])
                manual_cleaned_tag = HOLDING_STRATEGIES[code][0]  # è®°å½•ä¸‹æ¥ç”¨äºå»é‡
            elif code in holdings_map:
                t = f"æŒä»“/{name}"
                base_tags.append(t)
                manual_cleaned_tag = t
            else:
                # Fä½¬å…³æ³¨ - è¿›è¡Œæ·±åº¦æ¸…æ´—
                raw_note = f_lao_map[code]
                cleaned_note = clean_manual_tag(raw_note, has_zt_status)

                # é‡æ–°ç»„è£…
                final_manual = f"Fä½¬/{cleaned_note}" if cleaned_note != "å…³æ³¨" else "Fä½¬/å…³æ³¨"
                base_tags.append(final_manual)
                manual_cleaned_tag = final_manual
                
        # --- 2.1 é¾™è™æ¦œ & æ¸¸èµ„åˆ¤å®š (æ–°å¢) ---
        if code in lhb_codes:
            is_selected = True
            base_tags.append("ğŸ‰é¾™è™æ¦œ")
        
        if name in lhb_seat_map:
            is_selected = True
            # æ·»åŠ æ¸¸èµ„æ ‡ç­¾ (å·²å»é‡)
            # Sort order: Lock/Add (ğŸ”’/â•) > Buy (ğŸ’°) > Sell (ğŸƒ)
            def tag_sort_key(t):
                if t.startswith("ğŸ”’") or t.startswith("â•"): return 0
                if t.startswith("ğŸ’°"): return 1
                if t.startswith("ğŸƒ"): return 2
                return 3
                
            seat_tags = sorted(list(lhb_seat_map[name]), key=tag_sort_key)
            base_tags.extend(seat_tags)
        
        # --- 2.5 è¾¨è¯†åº¦/äººæ°”åˆ¤å®š (æ–°å¢) ---
        is_popular = False
        pop_reasons = set()
        
        # A. æ‰‹åŠ¨ç»´æŠ¤çš„äººæ°”è‚¡
        if code in manual_recognition_map or name in manual_recognition_map:
            is_popular = True
            
        # B. è‡ªåŠ¨åˆ¤å®šï¼š3è¿æ¿ä»¥ä¸Šé«˜æ ‡
        limit_days = item.get('limit_days', 0)
        if limit_days >= 3:
            is_popular = True
            # æ¿æ•°åé¢ä¼šè‡ªåŠ¨åŠ ï¼Œè¿™é‡Œä¸é‡å¤åŠ 
            
        # C. è‡ªåŠ¨åˆ¤å®šï¼šå¤§æˆäº¤é¢å‰æ’ (>=20äº¿)
        amount_val = item.get('amount', 0)
        if amount_val >= 20_0000_0000: # 20äº¿
            is_popular = True
            pop_reasons.add("æˆäº¤") 
        
        if is_popular:
            is_selected = True
            base_tags.append("â˜…äººæ°”")
            if pop_reasons:
                base_tags.extend(sorted(list(pop_reasons)))

        # --- 2.6 æ–­æ¿ååŒ… (æ–°ç­–ç•¥) ---
        # é€»è¾‘ï¼šæ˜¨æ—¥åœ¨ç‚¸æ¿æ±  + ä»Šæ—¥æ”¶çº¢ (æœ€å¥½çˆ†é‡)
        if code in broken_pool_map:
            # åªè¦æ˜¯çº¢ç›˜ï¼Œå°±çº³å…¥
            if pct > 0:
                is_selected = True
                
                # è®¡ç®—æ˜¯å¦çˆ†é‡
                yest_amt = broken_pool_map[code]['amount']
                curr_amt = item.get('amount', 0)
                
                label = "ğŸ”¥æ–­æ¿ååŒ…"
                if yest_amt > 10000 and curr_amt > yest_amt: # ç®€å•åˆ¤æ–­æˆäº¤é¢å¢åŠ 
                     label += "/çˆ†é‡"
                
                base_tags.append(label)

        # --- 2.7 Fä½¬ç„šè¯€æ¨¡å‹ (New) ---
        if code in history_map:
             f_tags = check_fen_jue(history_map[code])
             if f_tags:
                 base_tags.extend(f_tags)
                 is_selected = True # model selected it


        # --- 3. æ ‡ç­¾ç»„è£… ---

        # æ¶¨åœæ ‡ç­¾
        if is_zt:
            is_selected = True
            base_tags.append(zt_tag)

        # ç‚¸æ¿
        is_zb = False
        if "ç‚¸æ¿" in raw_tag_str:
            is_zb = True
        elif item.get('max_pct', 0) > 9.0 and pct < 9.0:
            is_zb = True

        if is_zb and pct > -7.0:
            is_selected = True
            base_tags.append("ğŸ‘€ç„šè¯€é¢„æœŸ/ç‚¸æ¿")

        # è·Œåœ
        if pct <= -9.0:
            is_selected = True
            base_tags.append("ğŸ“‰è·Œåœ/åšå¼ˆä¿®å¤")

        # å¤§é¢æˆäº¤ (è¡¥å½•)
        amount_yi = item.get('amount', 0) / 100000000.0
        if amount_yi > 20.0 and pct > 0:
            is_selected = True

        # --- 4. æœ€ç»ˆåˆå¹¶ ---
        if is_selected:
            # æå–æ¦‚å¿µ (å¹¶å»é‡)
            local_concepts = get_core_concepts_local(name, raw_tag_str)
            # å…³é”®ï¼šä»è‡ªåŠ¨æ¦‚å¿µä¸­å‰”é™¤å·²ç»åœ¨æ‰‹åŠ¨æ ‡ç­¾é‡Œå‡ºç°è¿‡çš„è¯
            unique_concepts = get_unique_concepts(manual_cleaned_tag, local_concepts)

            # ç‰¹æ®Šå½¢æ€ & æ¿å‹
            shape_tags, zt_type = check_special_shape(item)
            if zt_type: 
                # Avoid dup with 'xæ¿' tag? 
                # append zt_type to tags e.g. "3æ¿/Tå­—"
                # Need to find existing ZT tag and append logic, or just add independent tag
                base_tags.append(f"[{zt_type}]")
                item['limit_up_type'] = zt_type
                
            # --- Call Auction Ratio ---
            # Ratio = CallAmt / YestAmt
            yest_item = yest_full_data.get(code)
            call_auc_ratio = 0.0
            call_auc_amt = item.get('call_auction_amount', 0)
            if yest_item:
                y_amt = yest_item.get('amount', 0)
                if y_amt > 0:
                    call_auc_ratio = call_auc_amt / y_amt
            
            item['call_auction_ratio'] = round(call_auc_ratio, 3)

            # åˆå¹¶åˆ—è¡¨
            final_parts = []
            final_parts.extend(base_tags)
            if unique_concepts: final_parts.append(unique_concepts)
            final_parts.extend(shape_tags)

            # ç®€å•å»é‡ (é˜²æ­¢å®Œå…¨ä¸€æ ·çš„å­—ç¬¦ä¸²é‡å¤)
            seen_parts = set()
            clean_parts = []
            for p in final_parts:
                if p not in seen_parts:
                    clean_parts.append(p)
                    seen_parts.add(p)

            final_tag_str = "/".join(clean_parts)

            # å†æ¬¡æ¸…ç†å¯èƒ½äº§ç”Ÿçš„åŒæ–œæ 
            final_tag_str = final_tag_str.replace('//', '/')
            
            # --- æœ€ç»ˆ Tag ä¿®æ­£: ç¡®ä¿ ç„šè¯€ å…³é”®å­—æ˜¾çœ¼ ---
            final_tag_str = final_tag_str.replace("ğŸ”¥æ–­æ¿ååŒ…", "ğŸ”¥Aå¤§ç„šè¯€") 
            # If explicit "ğŸ”¥Aå¤§ç„šè¯€" from model, it will be kept. 
            
            row = {
                'sina_code': format_sina(code),
                'name': name,
                'tag': final_tag_str,
                'amount': item.get('amount', 0),
                'today_pct': pct,
                'turnover': item.get('turnover', 0),
                'open_pct': item.get('open_pct', 0),
                'price': item.get('price', 0),
                'pct_10': item.get('pct_10', 0),
                'link_dragon': get_link_dragon(code),
                'vol': item.get('vol', 0),
                'vol_prev': item.get('vol_prev', 0),
                'vol_ratio': item.get('vol_ratio', 0),
                'code': code
            }
            pool.append(row)

    # --- 4.5 å¼‚åŠ¨é£é™©è®¡ç®— (æ”¹ä¸ºè¯»å–æ‰‹åŠ¨æ–‡ä»¶) ---
    print(f"{Fore.MAGENTA}ğŸ” æ­£åœ¨åŠ è½½å¼‚åŠ¨é£é™©æ•°æ® (æ‰‹åŠ¨æ–‡ä»¶)...")
    try:
        # 1. å¯»æ‰¾æœ€æ–°çš„ risk_YYYYMMDD.csv
        input_dir = os.path.join(PROJECT_ROOT, 'data', 'input', 'risk')
        if not os.path.exists(input_dir):
            print(f"   âš ï¸ æœªæ‰¾åˆ°é£é™©æ–‡ä»¶å¤¹: {input_dir}")
            risk_files = []
        else:
            risk_files = [f for f in os.listdir(input_dir) if f.startswith('risk_') and f.endswith('.csv')]
        
        target_risk_file = None
        if risk_files:
            # Sort by date in filename risk_20260107.csv
            risk_files.sort(reverse=True)
            target_risk_file = os.path.join(input_dir, risk_files[0])
            print(f"   ğŸ“„ æ‰¾åˆ°æ–‡ä»¶: {risk_files[0]}")
        
        risk_map = {}
        if target_risk_file:
            try:
                # pandas read
                risk_df = pd.read_csv(target_risk_file)
                # Ensure columns exist
                # Expected: è‚¡ç¥¨åç§°,ç›‘ç®¡è§„åˆ™,å½“å‰ç´¯è®¡åç¦»å€¼,å¼‚åŠ¨è§¦å‘æ¡ä»¶,é£é™©ç­‰çº§,æ•°æ®æ—¥æœŸ
                # Map to: risk_level, risk_msg, trigger_next, risk_rule
                for _, row in risk_df.iterrows():
                    name = str(row['è‚¡ç¥¨åç§°']).strip()
                    # Parse Risk Msg for Values
                    msg = str(row.get('å½“å‰ç´¯è®¡åç¦»å€¼', ''))
                    
                    dev_10 = 0.0
                    dev_30 = 0.0
                    
                    # Extract percentage float
                    import re
                    match = re.search(r'(-?\d+\.?\d*)%', msg)
                    val = float(match.group(1)) if match else 0.0
                    
                    rule = str(row.get('ç›‘ç®¡è§„åˆ™', ''))
                    if '10æ—¥' in rule: dev_10 = val
                    if '30æ—¥' in rule: dev_30 = val
                    
                    risk_map[name] = {
                        'risk_level': str(row.get('é£é™©ç­‰çº§', 'ğŸŸ¢ Safe')),
                        'risk_msg': msg,
                        'risk_rule': rule,
                        'trigger_next': str(row.get('å¼‚åŠ¨è§¦å‘æ¡ä»¶', '')),
                        'deviation_val_10d': dev_10,
                        'deviation_val_30d': dev_30
                    }
            except Exception as e:
                print(f"{Fore.RED}âš ï¸ è¯»å–CSVå¤±è´¥: {e}")

        # 2. åˆå¹¶åˆ° pool
        matches = 0
        for p in pool:
            name = p['name']
            if name in risk_map:
                info = risk_map[name]
                p['risk_level'] = info['risk_level']
                p['risk_msg'] = info['risk_msg']
                p['risk_rule'] = info['risk_rule']
                p['trigger_next'] = info['trigger_next']
                p['deviation_val_10d'] = info['deviation_val_10d']
                p['deviation_val_30d'] = info['deviation_val_30d']
                matches += 1
            else:
                # Default safe
                p['risk_level'] = 'ğŸŸ¢ Safe'
                p['risk_msg'] = '-'
                p['trigger_next'] = '-'
                p['deviation_val_10d'] = 0.0
                p['deviation_val_30d'] = 0.0
                
        print(f"   âœ… æˆåŠŸåŒ¹é… {matches} åªæ ‡çš„é£é™©æ•°æ®")
        
    except Exception as e:
        print(f"{Fore.RED}âš ï¸ é£é™©æ•°æ®åŠ è½½å¼‚å¸¸: {e}")

    # --- 5. å¯¼å‡º ---
    if pool:
        df = pd.DataFrame(pool)
        df.sort_values(by='amount', ascending=False, inplace=True)

        cols = ['sina_code', 'name', 'tag', 'amount', 'today_pct', 'turnover', 'open_pct', 'price', 
                'risk_level', 'risk_msg', 'trigger_next', 'risk_rule', 'deviation_val_10d', 'deviation_val_30d',
                'call_auction_ratio', 'limit_up_type',  # New Cols
                'pct_10', 'link_dragon', 'vol', 'vol_prev', 'vol_ratio', 'code']
        for c in cols:
            if c not in df.columns: df[c] = 0
        df = df[cols]

        date_str = datetime.now().strftime("%Y%m%d")
        
        # æ”¹åŠ¨ï¼šç›´æ¥åœ¨ output ç›®å½•ç”Ÿæˆå¸¦æ—¥æœŸçš„æ–‡ä»¶ï¼Œæ–¹ä¾¿æŸ¥çœ‹
        dated_filename = f'strategy_pool_{date_str}.csv'
        dated_path = os.path.join(OUTPUT_DIR, dated_filename)
        latest_path = os.path.join(OUTPUT_DIR, 'strategy_pool.csv')

        df.to_csv(dated_path, index=False, encoding='utf-8-sig')
        
        # åŒæ—¶å¤åˆ¶ä¸€ä»½ä¸ºé€šç”¨åï¼Œä¾›å…¶ä»–è„šæœ¬è¯»å–
        shutil.copyfile(dated_path, latest_path)

        # --- å¯¼å‡ºå¤§ç›˜æ•°æ® JSON ---
        if market_loaded:
            market_json_path = os.path.join(OUTPUT_DIR, f'market_sentiment_{date_str}.json')
            try:
                final_json = md_manager.get_summary()
                final_json.update(market_stats) # Merge enhanced stats
                with open(market_json_path, 'w', encoding='utf-8') as f:
                    json.dump(final_json, f, indent=2, ensure_ascii=False)
                print(f"ğŸ“„ å¤§ç›˜æ•°æ®: {market_json_path}")
            except Exception as e:
                print(f"âŒ å¯¼å‡ºå¤§ç›˜JSONå¤±è´¥: {e}")

        print(f"\n{Fore.GREEN}ğŸ‰ ç¦»çº¿å¤ç›˜å®Œæˆï¼ç”Ÿæˆæ ‡çš„: {len(pool)} åª")
        print(f"ğŸ“„ æ—¥æœŸæ–‡ä»¶: {dated_path}")
        print(f"ğŸ“„ é€šç”¨æ–‡ä»¶: {latest_path} (å·²æ›´æ–°)")

    else:
        print(f"{Fore.RED}âŒ ç­›é€‰ç»“æœä¸ºç©ºã€‚")


if __name__ == "__main__":
    generate_strategy_pool()