import akshare as ak
import pandas as pd
import os
import shutil
from datetime import datetime
from colorama import init, Fore

init(autoreset=True)

# è·¯å¾„é…ç½®
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR))
OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'data', 'output')
LHB_DIR = os.path.join(OUTPUT_DIR, 'lhb') # New dedicated folder
ARCHIVE_DIR = os.path.join(OUTPUT_DIR, 'archive')

# çŸ¥åæ¸¸èµ„/å¸­ä½æ˜ å°„é…ç½®
# æ ¼å¼: 'æ¸¸èµ„æ ‡ç­¾': ['å…³é”®è¯1', 'å…³é”®è¯2']
# çŸ¥åæ¸¸èµ„/å¸­ä½æ˜ å°„é…ç½®
# æ ¼å¼: 'æ¸¸èµ„æ ‡ç­¾': ['å…³é”®è¯1', 'å…³é”®è¯2']
FAMOUS_SEATS = {
    #Top Yu
    'é™ˆå°ç¾¤': [
        'å¤§è¿é‡‘é©¬è·¯', 'ä¸­å›½é“¶æ²³è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å¤§è¿é‡‘é©¬è·¯',
        'å¤§è¿é»„æ²³è·¯', 'ä¸­å›½é“¶æ²³è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å¤§è¿é»„æ²³è·¯',
        'è‹å·ç•™å›­è·¯', 'ä¸œäºšå‰æµ·è¯åˆ¸æœ‰é™è´£ä»»å…¬å¸è‹å·ç•™å›­è·¯'
    ],
    'å‘¼å®¶æ¥¼': [
        'å‘¼å®¶æ¥¼', 'ä¸­ä¿¡è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸åŒ—äº¬å‘¼å®¶æ¥¼',
        'åŒ—äº¬ä¸­ä¿¡å¤§å¦', 'ä¸­ä¿¡å»ºæŠ•è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸åŒ—äº¬ä¸­ä¿¡å¤§å¦',
        'ä¸Šæµ·å‡¯æ»¨è·¯', 'ä¸­ä¿¡è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ä¸Šæµ·å‡¯æ»¨è·¯',
        'åŒ—äº¬ä¸œåŸåˆ†å…¬å¸', 'ä¸­ä¿¡å»ºæŠ•è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸åŒ—äº¬ä¸œåŸåˆ†å…¬å¸',
        'åŒ—äº¬å¹¿æ¸ é—¨å†…å¤§è¡—', 'ä¸­ä¿¡å»ºæŠ•è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸åŒ—äº¬å¹¿æ¸ é—¨å†…å¤§è¡—',
        'åŒ—äº¬æ€»éƒ¨', 'ä¸­ä¿¡è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸åŒ—äº¬æ€»éƒ¨',
        'åŒ—äº¬å»ºå¤–å¤§è¡—', 'å¹¿å‘è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸åŒ—äº¬å»ºå¤–å¤§è¡—'
    ],
    'æ–¹æ–°ä¾ ': [
        'è¥¿å®‰æœ±é›€å¤§è¡—', 'ä¸­ä¿¡è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸è¥¿å®‰æœ±é›€å¤§è¡—',
        'é™•è¥¿åˆ†å…¬å¸', 'å…´ä¸šè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸é™•è¥¿åˆ†å…¬å¸',
        'è¥¿å®‰æ›²æ±Ÿæ± å—è·¯', 'å›½æŠ•è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸è¥¿å®‰æ›²æ±Ÿæ± å—è·¯'
    ],
    'å…­ä¸€ä¸­è·¯': [
        'ç¦å·å…­ä¸€ä¸­è·¯', 'æ‹›å•†è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ç¦å·å…­ä¸€ä¸­è·¯',
        'å¤©æ´¥ä¸œä¸½å¼€å‘åŒº', 'åæ³°è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å¤©æ´¥ä¸œä¸½å¼€å‘åŒºäºŒçº¬è·¯', # Also äº¤æ˜“çŒ¿
        'æ·±åœ³æ·±å—å¤§é“', 'æ·±åœ³åˆ†å…¬å¸' # Sometimes
    ],
    'ç« ç›Ÿä¸»': [
        'ä¸Šæµ·æ±Ÿè‹è·¯', 'å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ä¸Šæµ·æ±Ÿè‹è·¯',
        'å®æ³¢æ±Ÿä¸œåŒ—è·¯' # 
    ],
    'çŸ¥æ˜¥è·¯': ['çŸ¥æ˜¥è·¯', 'åŒ—äº¬çŸ¥æ˜¥è·¯'], # Sell 3 in Goldwind
    'å…»å®¶': [
        'å®›å¹³å—è·¯', 'åé‘«è¯åˆ¸æœ‰é™è´£ä»»å…¬å¸ä¸Šæµ·å®›å¹³å—è·¯',
        'ä¸Šæµ·èŒ…å°è·¯', 'åé‘«è¯åˆ¸æœ‰é™è´£ä»»å…¬å¸ä¸Šæµ·èŒ…å°è·¯',
        'ä¸Šæµ·æ¾æ±Ÿ', 'åé‘«è¯åˆ¸æœ‰é™è´£ä»»å…¬å¸ä¸Šæµ·æ¾æ±Ÿ',
        'ä¸Šæµ·é™†å®¶å˜´', 'åé‘«è¯åˆ¸æœ‰é™è´£ä»»å…¬å¸ä¸Šæµ·é™†å®¶å˜´',
        'è¥¿å®‰äºŒç¯', 'åé‘«è¯åˆ¸æœ‰é™è´£ä»»å…¬å¸è¥¿å®‰äºŒç¯' # Sometimes used
    ],
    'ä¸Šå¡˜è·¯': ['ä¸Šå¡˜è·¯', 'è´¢é€šè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ­å·ä¸Šå¡˜è·¯', 'ä½“è‚²é¦†è·¯', 'è´¢é€šè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ­å·ä½“è‚²é¦†è·¯'],
    'ä½œæ‰‹æ–°ä¸€': [
        'å—äº¬å¤ªå¹³å—è·¯', 'å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å—äº¬å¤ªå¹³å—è·¯', 
        'å—äº¬é‡‘èåŸ', 
        'é‡åº†è§£æ”¾ç¢‘', 'å›½æ³°æµ·é€šè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸é‡åº†è§£æ”¾ç¢‘', 'å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸é‡åº†è§£æ”¾ç¢‘' # User noted, usually Zuoshou
    ],
    'å°é³„é±¼': ['å—äº¬å¤§é’Ÿäº­', 'å—äº¬è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å—äº¬å¤§é’Ÿäº­', 'ä¸Šæµ·ä¸œæ–¹è·¯', 'å¹¿å‘è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ä¸Šæµ·ä¸œæ–¹è·¯'],
    'æ¯›è€æ¿': ['åŒ—äº¬åŒ—ä¸‰ç¯ä¸œè·¯', 'æˆéƒ½å—ä¸€ç¯è·¯'],
    
    # New Additions
    '92ç§‘æ¯”': ['æ³°å·é¼“æ¥¼å—è·¯', 'å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ³°å·é¼“æ¥¼å—è·¯', 'å—äº¬å¤©å…ƒä¸œè·¯', 'å…´ä¸šè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å—äº¬å¤©å…ƒä¸œè·¯'],
    'æ¶ˆé—²æ´¾': ['å®œæ˜Œçç è·¯', 'å›½æ³°å›å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å®œæ˜Œçç è·¯', 'å®œæ˜Œæ²¿æ±Ÿå¤§é“', 'å›½æ³°æµ·é€šè¯åˆ¸å…¬å®œæ˜Œæ²¿æ±Ÿå¤§é“è¥ä¸šéƒ¨'], # Added Yanjiang
    'ä½™å“¥': ['ç›¸åŸå¤§é“', 'å…‰å¤§è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸è‹å·ç›¸åŸå¤§é“', 'ä¸œå´è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸è‹å·ç›¸åŸå¤§é“', 'å®æ³¢æ²™æ»©è·¯', 'ä½™å§šèˆœæ°´å—è·¯', 'å®æ³¢æµ·æ™åŒ—è·¯', 'å¹³å®‰è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å®æ³¢æµ·æ™åŒ—è·¯'], # Added Soochow Xiangcheng
    'èµµè€å“¥': ['ç»å…´', 'ä¸­å›½é“¶æ²³è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ç»å…´', 'ç»å…´è§£æ”¾åŒ—è·¯'],
    'ä¸­å±±ä¸œè·¯': ['ä¸­å±±ä¸œè·¯', 'ä¸Šæµ·æ¾æ±ŸåŒºä¸­å±±ä¸œè·¯', 'å›½æ³°æµ·é€šè¯åˆ¸ä¸Šæµ·æ¾æ±ŸåŒºä¸­å±±ä¸œè·¯è¥ä¸šéƒ¨'], # New from news
    'å®æ³¢æ¡‘ç”°è·¯': ['å®æ³¢æ¡‘ç”°è·¯', 'å›½ç››è¯åˆ¸æœ‰é™è´£ä»»å…¬å¸å®æ³¢æ¡‘ç”°è·¯'],
    'ä½›å±±ç³»': ['ä½›å±±ç»¿æ™¯è·¯', 'å…‰å¤§è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸ä½›å±±ç»¿æ™¯è·¯', 'ä½›å±±å­£åå…­è·¯'],
    'å’Œå¹³è·¯': ['éå±±å’Œå¹³è·¯', 'ä¸­ä¿¡è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸éå±±å’Œå¹³è·¯'], # Big buyer usually
    'äº¤æ˜“çŒ¿': ['å¤©æ´¥ä¸œä¸½å¼€å‘åŒº', 'åæ³°è¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸å¤©æ´¥ä¸œä¸½å¼€å‘åŒºäºŒçº¬è·¯'], # Often same as 61
    'æ€æ˜å—è·¯': ['ä¸œèè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ¹–åŒ—åˆ†å…¬å¸', 'ä¸œäºšå‰æµ·è¯åˆ¸æœ‰é™è´£ä»»å…¬å¸ä¸Šæµ·åˆ†å…¬å¸'],
    
    # Groups
    'æ‹‰è¨å¤©å›¢': [
        'æ‹‰è¨å›¢ç»“è·¯', 'ä¸œæ–¹è´¢å¯Œè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ‹‰è¨å›¢ç»“è·¯',
        'æ‹‰è¨ä¸œç¯è·¯', 'ä¸œæ–¹è´¢å¯Œè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ‹‰è¨ä¸œç¯è·¯',
        'æ‹‰è¨é‡‘èåŸ', 'ä¸œæ–¹è´¢å¯Œè¯åˆ¸è‚¡ä»½æœ‰é™å…¬å¸æ‹‰è¨é‡‘èåŸ'
    ],
    'åŒ—å‘': ['æ·±è‚¡é€š', 'æ²ªè‚¡é€š', 'é¦™æ¸¯ä¸­å¤®ç»“ç®—æœ‰é™å…¬å¸'],
    'æœºæ„': ['æœºæ„ä¸“ç”¨']
}

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LHB_DIR, exist_ok=True)
os.makedirs(ARCHIVE_DIR, exist_ok=True)


def fetch_famous_seats(date_str=None):
    """
    è·å–çŸ¥åæ¸¸èµ„æ´»è·ƒæ•°æ® (é€šè¿‡éå†å½“æ—¥é¾™è™æ¦œæ ‡çš„è¯¦æƒ…)
    """
    if not date_str:
        date_str = datetime.now().strftime("%Y%m%d")
        
    print(f"{Fore.MAGENTA}ğŸ•µï¸ å¼€å§‹è¿½è¸ªçŸ¥åæ¸¸èµ„ (æ·±åº¦æ‰«æ): {date_str}")
    
    # 1. è¯»å–å½“æ—¥é¾™è™æ¦œæ ‡çš„åˆ—è¡¨
    lhb_file = os.path.join(LHB_DIR, f"lhb_{date_str}.csv")
    if not os.path.exists(lhb_file):
        print(f"{Fore.YELLOW}âš ï¸ æœªæ‰¾åˆ°å½“æ—¥é¾™è™æ¦œåŸºç¡€æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ fetch_lhb_data")
        return

    try:
        df_lhb = pd.read_csv(lhb_file, dtype={'ä»£ç ': str})
        if df_lhb.empty: return
        
        codes = df_lhb['ä»£ç '].unique().tolist()
        print(f"   ğŸ“‹ å¾…æ‰«ææ ‡çš„: {len(codes)} åª")
        
        hits = [] # {æ¸¸èµ„, è¥ä¸šéƒ¨, è‚¡ç¥¨, æ“ä½œ, é‡‘é¢}
        
        from tqdm import tqdm
        for code in tqdm(codes, desc="Scanning Seats"):
            try:
                # è·å–ä¸ªè‚¡è¯¦æƒ…
                # stock_lhb_stock_detail_em: ä¸œæ–¹è´¢å¯Œ-ä¸ªè‚¡é¾™è™æ¦œè¯¦æƒ…
                # Fix: Must fetch both 'ä¹°å…¥' and 'å–å‡º' lists to get complete data
                try:
                    df_buy = ak.stock_lhb_stock_detail_em(symbol=code, date=date_str, flag="ä¹°å…¥")
                    df_sell = ak.stock_lhb_stock_detail_em(symbol=code, date=date_str, flag="å–å‡º")
                    
                    df_detail = pd.concat([df_buy, df_sell], ignore_index=True)
                    # Deduplicate based on Branch and Type (as one branch might appear in multiple list types, e.g., 3-day and 1-day)
                    # But merging duplicates with same values is fine. 
                    # Warning: valid to have same branch in 1-day AND 3-day list (different 'ç±»å‹'). 
                    # If same branch/type appears in buy and sell list, it is identical.
                    df_detail = df_detail.drop_duplicates(subset=['äº¤æ˜“è¥ä¸šéƒ¨åç§°', 'ç±»å‹'])
                    
                except Exception as e:
                    # print(f"Error fetching detail for {code}: {e}")
                    continue

                if df_detail.empty: continue
                
                # Check columns to ensure we access correctly
                # Expected: è¥ä¸šéƒ¨åç§°, ä¹°å…¥é‡‘é¢, å–å‡ºé‡‘é¢ (values usually in float or string)
                if code == "002413":
                    print(f"DEBUG {code} COLUMNS:", df_detail.columns.tolist())
                    print(df_detail.head(2)) 
                
                stock_name = df_lhb[df_lhb['ä»£ç '] == code]['åç§°'].values[0]
                
                for _, row in df_detail.iterrows():
                    # Column name might be 'è¥ä¸šéƒ¨åç§°' or 'äº¤æ˜“è¥ä¸šéƒ¨åç§°'
                    branch = str(row.get('è¥ä¸šéƒ¨åç§°') or row.get('äº¤æ˜“è¥ä¸šéƒ¨åç§°', ''))
                    branch = branch.strip()
                    
                    # Amt might be string with commas
                    try:
                        buy_amt = float(row.get('ä¹°å…¥é‡‘é¢', 0))
                    except: buy_amt = 0
                    
                    try:
                        sell_amt = float(row.get('å–å‡ºé‡‘é¢', 0))
                    except: sell_amt = 0
                    
                    # åŒ¹é…çŸ¥åæ¸¸èµ„
                    for label, keywords in FAMOUS_SEATS.items():
                        for kw in keywords:
                            if kw in branch:
                                # å‘½ä¸­
                                action_type = "è§‚æœ›"
                                
                                # è§£ææ¦œå•ç±»å‹ (æ—¥æ¦œ vs 3æ—¥æ¦œ)
                                lhb_type = row.get('ç±»å‹', '')
                                time_tag = "æ—¥" # Default
                                if "ä¸‰" in lhb_type or "3" in lhb_type:
                                    time_tag = "3æ—¥"
                                elif "ä¸¥é‡" in lhb_type: # ä¸¥é‡å¼‚å¸¸æ³¢åŠ¨ usually covers longer period (e.g. 10 days) or specific check
                                    time_tag = "ä¸¥é‡å¼‚åŠ¨"
                                
                                # é˜ˆå€¼è°ƒæ•´: é¿å…å¾®é‡ä¹°å…¥è¢«è¯¯åˆ¤ä¸ºåšT
                                # 1. æ˜¾è‘—æ€§åˆ¤æ–­
                                is_buy_sig = buy_amt > 100000  # 10ä¸‡
                                is_sell_sig = sell_amt > 100000 # 10ä¸‡
                                
                                if is_buy_sig and not is_sell_sig:
                                    action_type = "ä¹°å…¥"
                                elif is_sell_sig and not is_buy_sig:
                                    action_type = "å–å‡º"
                                elif is_buy_sig and is_sell_sig:
                                    # åŒå‘éƒ½æœ‰ï¼Œçœ‹æ¯”ä¾‹
                                    if buy_amt > sell_amt * 5:
                                        action_type = "ä¹°å…¥" # ä¹°å…¥è¿œå¤§äºå–å‡º
                                    elif sell_amt > buy_amt * 5:
                                        action_type = "å–å‡º" # å–å‡ºè¿œå¤§äºä¹°å…¥ (å¦‚ é™ˆå°ç¾¤å–é›·ç§‘ 1.1äº¿ vs ä¹° 200ä¸‡)
                                    else:
                                        action_type = "åšT"
                                else:
                                    action_type = "è§‚æœ›"
                                
                                hits.append({
                                    'æ¸¸èµ„æ ‡ç­¾': label,
                                    'è¥ä¸šéƒ¨åç§°': branch,
                                    'è‚¡ç¥¨ä»£ç ': code,
                                    'è‚¡ç¥¨åç§°': stock_name,
                                    'æ“ä½œ': action_type,
                                    'ä¹°å…¥é‡‘é¢': buy_amt,
                                    'å–å‡ºé‡‘é¢': sell_amt,
                                    'æ¦œå•æ ‡ç­¾': time_tag
                                })
                                break # Match one label only
            except Exception as e:
                # print(f"Error scanning {code}: {e}")
                continue
                
        # --- Locking Position Detection (Suocang) ---
        # Logic: If (Seat, Stock) in Yesterday's Famous Buy List AND Stock in Today's LHB AND Seat NOT in Today's Sell List -> Locked
        
        try:
            # 1. Find previous famous file
            # Simple lookback for now
            import datetime as dt
            curr_date = datetime.strptime(date_str, "%Y%m%d")
            prev_file = None
            for i in range(1, 5): # Check back 4 days for previous trading day
                prev_d_str = (curr_date - dt.timedelta(days=i)).strftime("%Y%m%d")
                p_path = os.path.join(LHB_DIR, f"lhb_famous_{prev_d_str}.csv")
                if os.path.exists(p_path):
                    prev_file = p_path
                    print(f"   ğŸ” å¯¹æ¯”æ˜¨æ—¥æ•°æ®: {prev_d_str}")
                    break
            
            if prev_file:
                df_prev = pd.read_csv(prev_file)
                # Prepare today's sell set: {(Label, Stock)}
                today_sell_set = set()
                for h in hits:
                    if "å–" in h['æ“ä½œ'] or "åšT" in h['æ“ä½œ']:
                        today_sell_set.add((h['æ¸¸èµ„æ ‡ç­¾'], h['è‚¡ç¥¨åç§°']))
                
                # Check previous buys
                for _, row in df_prev.iterrows():
                    p_label = row['æ¸¸èµ„æ ‡ç­¾']
                    p_buys = str(row['ä¹°å…¥è‚¡ç¥¨'])
                    if pd.isna(p_buys) or not p_buys.strip(): continue
                    
                    # Buy string might be "StockA StockB(1.2äº¿)"
                    import re
                    # Extract stock names from "StockA(1äº¿) StockB"
                    # Simple split by space
                    p_stocks_raw = p_buys.split(' ')
                    for s_raw in p_stocks_raw:
                        if not s_raw: continue
                        # Remove amount info like (1.2äº¿)
                        s_name = re.sub(r'\(.*?\)', '', s_raw)
                        s_name = s_name.strip()
                        
                        # Check if this stock is in TODAY'S LHB List (df_lhb)
                        if s_name in df_lhb['åç§°'].values:
                            # Start check
                            has_sold = (p_label, s_name) in today_sell_set
                            has_bought_today = False
                            for h in hits:
                                if h['æ¸¸èµ„æ ‡ç­¾'] == p_label and h['è‚¡ç¥¨åç§°'] == s_name and ("ä¹°" in h['æ“ä½œ'] or "åšT" in h['æ“ä½œ']):
                                    has_bought_today = True
                                    
                            if not has_sold:
                                status = "ğŸ”’ é”ä»“"
                                if has_bought_today:
                                    status = "â• åŠ ä»“" # Bought and didn't sell
                                
                                already_recorded = False
                                for h in hits:
                                    if h['æ¸¸èµ„æ ‡ç­¾'] == p_label and h['è‚¡ç¥¨åç§°'] == s_name:
                                        # Update existing hit special status if needed, but easier to just skip adding duplicative "Lock" entry
                                        # Only add if completely missing from today's active list
                                        already_recorded = True 
                                        break
                                        
                                if not already_recorded:
                                    hits.append({
                                        'æ¸¸èµ„æ ‡ç­¾': p_label,
                                        'è¥ä¸šéƒ¨åç§°': f"{p_label}å¸­ä½(æ¨æµ‹)",
                                        'è‚¡ç¥¨ä»£ç ': "", 
                                        'è‚¡ç¥¨åç§°': s_name,
                                        'æ“ä½œ': status,
                                        'ä¹°å…¥é‡‘é¢': 0,
                                        'å–å‡ºé‡‘é¢': 0,
                                        'æ¦œå•æ ‡ç­¾': "æ—¥" # Lock means checking against today's status, usually implies keeping daily position
                                    })
        except Exception as e:
            print(f"Error checking locks: {e}")

        # æ•´ç†è¾“å‡º
        if hits:
            # Aggregation v2:
            # 1. Group by (Label, Branch, Stock, Tag) -> Take MAX amounts (Dedup 1-day/3-day for same branch IF SAME TAG)
            # 2. Group by (Label, Stock, Tag) -> SUM amounts (Combine multiple branches for same investor)
            
            # Step 1: Branch Level Max (Per Tag)
            # If a branch appears in Daily list, we take its max for Daily.
            # If it appears in 3-Day list, we take its max for 3-Day.
            branch_map = {} # (Label, Branch, Stock, Tag) -> {'buy': max_b, 'sell': max_s, 'status': s}
            
            for h in hits:
                lbs_key = (h['æ¸¸èµ„æ ‡ç­¾'], h['è¥ä¸šéƒ¨åç§°'], h['è‚¡ç¥¨åç§°'].strip(), h['æ¦œå•æ ‡ç­¾'])
                if lbs_key not in branch_map:
                    branch_map[lbs_key] = {'buy': 0, 'sell': 0, 'special_status': None}
                
                curr = branch_map[lbs_key]
                curr['buy'] = max(curr['buy'], h['ä¹°å…¥é‡‘é¢'])
                curr['sell'] = max(curr['sell'], h['å–å‡ºé‡‘é¢'])
                if "é”ä»“" in h['æ“ä½œ'] or "åŠ ä»“" in h['æ“ä½œ']:
                    curr['special_status'] = h['æ“ä½œ']

            # Step 2: Investor Level Sum (Per Tag)
            final_map = {} # (Label) -> { (Stock, Tag): {'buy': sum_b, 'sell': sum_s, 'status': ...} }
            
            for (label, branch, stock, tag), vals in branch_map.items():
                if label not in final_map: final_map[label] = {}
                st_key = (stock, tag)
                if st_key not in final_map[label]: final_map[label][st_key] = {'buy': 0, 'sell': 0, 'statuses': set()}
                
                f_curr = final_map[label][st_key]
                f_curr['buy'] += vals['buy']
                f_curr['sell'] += vals['sell']
                if vals['special_status']:
                    f_curr['statuses'].add(vals['special_status'])
            
            # Step 3: Format Rows
            final_rows = []
            for label, item_dict in final_map.items():
                buy_strs = []
                sell_strs = []
                
                # Sort items: First by Stock Name, then by Tag (Daily before 3-Day)
                # item_dict keys are (Stock, Tag)
                sorted_keys = sorted(item_dict.keys(), key=lambda x: (x[0], x[1] != 'æ—¥')) # 'æ—¥' comes first
                
                for s_name, tag in sorted_keys:
                    vals = item_dict[(s_name, tag)]
                    b_amt = vals['buy']
                    s_amt = vals['sell']
                    
                    s_display = s_name
                    # Append Tag if not 'æ—¥'
                    if tag != 'æ—¥':
                        s_display += f"/{tag}"
                        
                    if vals['statuses']:
                        # prioritizing Lock status display
                        status_str = "/".join(list(vals['statuses']))
                        s_display = f"{s_display}({status_str})"
                    
                    # Formatting check: Show if Buy > 100k OR if "Lock" status (even if buy=0)
                    has_buy_sig = b_amt > 100000
                    has_sell_sig = s_amt > 100000
                    is_special = len(vals['statuses']) > 0
                    
                    if has_buy_sig or (is_special and "é”ä»“" not in str(vals['statuses'])): 
                        amt_str = ""
                        if b_amt > 100000:
                            amt_str = f"({b_amt/10000:.0f}ä¸‡)"
                            if b_amt > 100000000:
                                amt_str = f"({b_amt/100000000:.1f}äº¿)"
                        
                        buy_strs.append(f"{s_display}{amt_str}")

                    elif is_special and "é”ä»“" in str(vals['statuses']):
                        buy_strs.append(f"{s_display}")

                    if has_sell_sig:
                        amt_str = f"({s_amt/10000:.0f}ä¸‡)"
                        if s_amt > 100000000:
                            amt_str = f"({s_amt/100000000:.1f}äº¿)"
                        sell_strs.append(f"{s_display}{amt_str}")

                if not buy_strs and not sell_strs:
                    continue
                
                final_rows.append({
                    'æ¸¸èµ„æ ‡ç­¾': label,
                    'è¥ä¸šéƒ¨åç§°': "å¤šå¸­ä½/èšåˆ", 
                    'ä¹°å…¥è‚¡ç¥¨': " ".join(buy_strs),
                    'å–å‡ºè‚¡ç¥¨': " ".join(sell_strs),
                    'ä¸Šæ¦œæ¬¡æ•°': len(buy_strs) + len(sell_strs)
                })

            # Sort by Label
            final_rows.sort(key=lambda x: x['æ¸¸èµ„æ ‡ç­¾'])
            
            df_res = pd.DataFrame(final_rows)
            file_path = os.path.join(LHB_DIR, f"lhb_famous_{date_str}.csv")
            df_res.to_csv(file_path, index=False, encoding='utf-8-sig')
            
            latest_path = os.path.join(LHB_DIR, "lhb_famous_latest.csv")
            shutil.copyfile(file_path, latest_path)
            
            print(f"{Fore.GREEN}âœ… æ·±åº¦æ‰«æå®Œæˆï¼Œå‘ç° {len(final_rows)} ä¸ªæ´»è·ƒå¸­ä½")
            # Print preview
            for _, r in df_res.iterrows():
                msg = f"   ğŸ”¥ [{r['æ¸¸èµ„æ ‡ç­¾']}]"
                if r['ä¹°å…¥è‚¡ç¥¨']: msg += f" | ä¹°: {r['ä¹°å…¥è‚¡ç¥¨']}"
                if r['å–å‡ºè‚¡ç¥¨']: msg += f" | å–: {r['å–å‡ºè‚¡ç¥¨']}"
                print(msg)
                
        else:
             print(f"{Fore.CYAN}ğŸ¤· æ— çŸ¥åæ¸¸èµ„ä¸Šæ¦œ")

    except Exception as e:
        print(f"{Fore.RED}âš ï¸ æ‰«æå¤±è´¥: {e}")


def fetch_lhb_data(date_str=None):
    """
    è·å–æŒ‡å®šæ—¥æœŸçš„é¾™è™æ¦œè¯¦æƒ…
    date_str: YYYYMMDD
    """
    if not date_str:
        date_str = datetime.now().strftime("%Y%m%d")
        
    print(f"{Fore.CYAN}ğŸš€ å¼€å§‹è·å–é¾™è™æ¦œæ•°æ®: {date_str}")
    
    try:
        # ä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ¥å£ï¼Œæ•°æ®è¾ƒå…¨
        # start_date å’Œ end_date è®¾ç½®ä¸ºåŒä¸€å¤©
        df = ak.stock_lhb_detail_em(start_date=date_str, end_date=date_str)
        
        if df.empty:
            print(f"{Fore.RED}âŒ è¯¥æ—¥æœŸæ— é¾™è™æ¦œæ•°æ®")
            return None
            
        print(f"{Fore.GREEN}âœ… è·å–æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½• (å«åŒä¸€è‚¡ç¥¨å¤šæ¡ä¸Šæ¦œè®°å½•)")
        return df
        
    except Exception as e:
        print(f"{Fore.RED}âš ï¸ è·å–å¤±è´¥: {e}")
        return None

def process_and_save(df, date_str):
    """
    æ¸…æ´—å¹¶ä¿å­˜æ•°æ®
    """
    if df is None or df.empty: return

    # 1. ç®€å•æ¸…æ´— / é‡å‘½å
    # åŸå§‹åˆ—é€šå¸¸åŒ…å«: åºå·, ä»£ç , åç§°, è§£è¯», æ”¶ç›˜ä»·, æ¶¨è·Œå¹…, é¾™è™æ¦œå‡€ä¹°é¢, é¾™è™æ¦œä¹°å…¥é¢, é¾™è™æ¦œå–å‡ºé¢, é¾™è™æ¦œæˆäº¤é¢, å¸‚åœºæ€»æˆäº¤é¢, å‡€ä¹°é¢å æ€»æˆäº¤æ¯”, æˆäº¤é¢å æ€»æˆäº¤æ¯”, æ¢æ‰‹ç‡, ä¸Šæ¦œåŸå› 
    
    # æŒ‰ç…§æƒ¯ä¾‹ï¼Œæ•´ç†ä¸€ä¸‹åˆ—é¡ºåºï¼ŒæŠŠé‡è¦çš„æ”¾å‰é¢
    # å¿…é¡»å­˜åœ¨çš„åˆ—æ˜ å°„ (æ ¹æ® debug_lhb.py çš„è§‚å¯Ÿ)
    # å‡è®¾ akshare è¿”å›çš„æ ‡å‡†ä¸­æ–‡åˆ—å
    
    # å°è¯•ç­›é€‰å’Œæ’åºåˆ—
    target_cols = [
        'ä»£ç ', 'åç§°', 'ä¸Šæ¦œåŸå› ', 
        'æ”¶ç›˜ä»·', 'æ¶¨è·Œå¹…', 'æ¢æ‰‹ç‡',
        'é¾™è™æ¦œå‡€ä¹°é¢', 'é¾™è™æ¦œä¹°å…¥é¢', 'é¾™è™æ¦œå–å‡ºé¢', 'é¾™è™æ¦œæˆäº¤é¢',
        'å¸‚åœºæ€»æˆäº¤é¢', 'å‡€ä¹°é¢å æ€»æˆäº¤æ¯”'
    ]
    
    # åªæœ‰å­˜åœ¨çš„åˆ—æ‰ä¿ç•™
    available_cols = [c for c in target_cols if c in df.columns]
    df = df[available_cols]
    
    # æ’åº: æŒ‰ç…§ é¾™è™æ¦œå‡€ä¹°é¢ é™åº (æ³¨æ„å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è½¬æ¢)
    if 'é¾™è™æ¦œå‡€ä¹°é¢' in df.columns:
        # å¯èƒ½æ˜¯ numeric æˆ–è€…æ˜¯ objectï¼Œakshare è¿™ä¸ªæ¥å£é€šå¸¸è¿”å› object å¸¦ç€å•ä½? 
        # è¿™é‡Œè¿˜æ˜¯åšä¸ªé˜²é”™å¤„ç†
        # è§‚å¯Ÿ debug è¾“å‡ºï¼Œakshare em æ¥å£è¿”å›é€šå¸¸æ˜¯ float
        pass

    # ä¿å­˜æ–‡ä»¶
    filename = f"lhb_{date_str}.csv"
    filepath = os.path.join(LHB_DIR, filename)
    
    try:
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        print(f"ğŸ“„ å·²ä¿å­˜: {filepath}")
        
        # å¤åˆ¶ä¸º latest
        latest_path = os.path.join(LHB_DIR, "lhb_latest.csv")
        shutil.copyfile(filepath, latest_path)
        print(f"ğŸ“„ å·²æ›´æ–°: {latest_path}")
        
    except Exception as e:
        print(f"{Fore.RED}âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")


def get_recent_trade_dates(days=5):
    """
    è·å–æœ€è¿‘ N ä¸ªäº¤æ˜“æ—¥ (åŒ…æ‹¬ä»Šå¤©å¦‚æœä»Šå¤©ä¹Ÿæ˜¯äº¤æ˜“æ—¥)
    è¿”å›æ ¼å¼: ['20230101', '20230102', ...] (ä»æ—§åˆ°æ–°)
    """
    try:
        # fetch trade dates
        df = ak.tool_trade_date_hist_sina()
        df['trade_date'] = pd.to_datetime(df['trade_date'])
        
        today = datetime.now().date()
        # Filter dates <= today
        past_dates = df[df['trade_date'].dt.date <= today]
        
        if past_dates.empty:
            return [today.strftime("%Y%m%d")]

        # Get last N dates
        recent = past_dates.iloc[-days:]['trade_date'].dt.strftime("%Y%m%d").tolist()
        return recent
    except Exception as e:
        print(f"{Fore.RED}âš ï¸ è·å–äº¤æ˜“æ—¥å†å¤±è´¥: {e}")
        # Fallback: return today and yesterday
        today_str = datetime.now().strftime("%Y%m%d")
        from datetime import timedelta
        yest_str = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")
        return [yest_str, today_str]

if __name__ == "__main__":
    # æ™ºèƒ½æŸ¥æ‰¾æœ€è¿‘çš„é¾™è™æ¦œæ•°æ®
    # ç­–ç•¥: è·å–æœ€è¿‘ 3 ä¸ªäº¤æ˜“æ—¥ï¼Œå€’åºæŸ¥æ‰¾ (æœ€æ–° -> æœ€æ—§)
    # è¿™æ ·å¯ä»¥å¤„ç†å‘¨æœ«ã€èŠ‚å‡æ—¥ã€æ™šé—´æœªæ›´æ–°ç­‰æƒ…å†µ
    
    print(f"{Fore.CYAN}ğŸ“… æ­£åœ¨ç¡®å®šæœ€è¿‘çš„äº¤æ˜“æ—¥æ•°æ®...")
    
    candidates = get_recent_trade_dates(days=3)
    # Reverse to check latest first
    candidates.reverse()
    
    found_date = None
    
    for date_str in candidates:
        print(f"   ğŸ‘‰ å°è¯•æ—¥æœŸ: {date_str}")
        df = fetch_lhb_data(date_str)
        if df is not None and not df.empty:
            found_date = date_str
            process_and_save(df, date_str)
            fetch_famous_seats(date_str)
            break
            
    if not found_date:
        print(f"{Fore.RED}âŒ æœ€è¿‘ 3 ä¸ªäº¤æ˜“æ—¥å‡æœªè·å–åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç¨åå†è¯•ã€‚")

